\subsection{Не класичний регресійний аналіз}
Розглянемо тепер випадки, коли деякі з припущень моделі, яку ми розглядали не виконуються.
\subsubsection{Випадок корельованих збурень}
Для нашої моделі $ y = X \alpha + e$, де в класичному регресійному аналізі $e \sim N(\vec0, \sigma^2E)$. \\

Нехай тепер це не так, а $e \sim N(\vec0, R)$, де $R > 0$ -- кореляційна матриця. Спробуємо і для цього випадку отримати оцінку з тими ж хорошими властивостями, як і в класичному випадку. \\

\[ R^{-1/2}y = R^{-1/2}X\alpha + R^{-1/2}e, \] тоді, згідно властивості лінійного перетворення нормально розподілених випадкових величин $R^{-1/2}e \sim N(\vec0, E)$, тобто тут помилки уже будуть незалежними. \\

Ввівши позначення отримаємо нову модель $\widetilde{y} = \widetilde{X}\alpha + \widetilde{e}$ причому $\widetilde{e}\sim N(\vec0,R)$ і тоді будуємо оцінку класичним методом найменших квадратів : \begin{multline*} \widehat{\alpha} = (\widetilde{X}^*\widetilde{X})^{-1}\widetilde{X}^T\widetilde{y}=(X^TR^{-1/2}R^{-1/2}X)^{-1}X^TR^{-1/2}R^{-1/2}y=\\
=(X^TR^{-1}X)^{-1}X^TR^{-1}y. \end{multline*}
Цю оцінку називають \textit{Марківською}. \\

На неї розповсюджуються всі раніше сформульовані властивості, все прекрасно, але радіти рано: треба знати матрицю $R$, а на практиці вона зазвичай невідома. Часто можна знайти лише деяке наближення до неї, а тоді наближене значення $R^{-1}$ може бути дуже далеко від реального значення, а потім ще й до $X^TR^{-1}X$ треба шукати обернену... \\

\textbf{Марківська оцінка має наступні властивості}:
\begin{enumerate}
	\item $\widehat{\alpha}\sim N(\alpha, (X^TR^{-1}X)^{-1})$. 
	\item $\widehat{\alpha}$ ефективна на класі всіх незміщених оцінок.
	\item Якщо кількість вимірів зростає (тобто залежить від об’єму), тоді \[ \widehat{\alpha}(N) = (X_N^TR_N^{-1}X_N)^{-1}X_N^TR_N^{-1}y_N \] є сильно слушною тоді і тільки тоді, коли \[ (X_N^TR_N^{-1}X_N)^{-1} \xrightarrow[N\to\infty]{} O. \]
\end{enumerate}
Марківську оцінку можна розглядати також як \textit{розв’язок наступної оптимізаційної задачі}: \[ \widehat{\alpha} = \argmin_\alpha \|e\|_{R^{-1}}^2. \]
Ця оцінка -- це фактично один із прикладів, коли теорія -- теорією, а практика -- практикою.
\subsubsection{Оцінки параметрів в умовах мультиколінеарності}
Для $y=X\alpha+e$ в класичних припущеннях ми вважали, що $\rang X = p$, тобто повний по стовпчиках ($p = \dim \alpha$). Знімемо це припущення. \\

Нехай $\rang X = p - q$, $q \ge 1$. Тоді, як результат, отримаємо, що класична оцінка просто не існує, оскільки матриця $XX^T$ буде виродженою. Фактично це означає, що оцінка методом найменших квадратів буде не єдиною, і можна вибрати ту, яка найкраща для нас. \\

Нехай $\exists a_i: Xa_i=\vec0$. \\

В цьому випадку кажуть, що ми знаходимось в умовах строгої мультиколінеарності. Якщо ж ця умова  виконується тільки приблизно, то кажуть, що ми знаходимось в умовах мультиколінеарності. \\

Проаналізуємо ці випадки:
\begin{enumerate}
	\item Строга мультиколінеарність: оцінка не єдина, множина оцінок є розв’язком системи \[X^TX\widehat{\alpha}=X^Ty.\] В принципі тут можна використати псевдообернення, але це виходить за рамки нашого курсу.
	\item Мультиколінеарність: класична оцінка існує і вона єдина, але це теорія, а на практиці матриця $X^TX$ виходить погано обумовленою (існують власні числа близькі нуля), тому оцінка \textit{нестійка} та \textit{малоефективна} ($\sigma(X^TX)^{-1}$ може бути дуже великою).
\end{enumerate}
Є різні підходи до розв’язання цієї проблеми. Викладемо такий:
\subsection{Гребеневі оцінки (ridge-оцінки)}
Їх запропонував Херл в 1962р. \textbf{Ідея проста}: вводимо параметр $\theta$ і збурюємо $X^TX$:
\[ \widehat{\alpha}(\theta)=(X^TX+\theta E)^{-1}X^Ty,\] де $\theta > 0$, але досить мале.\\

Введення цього зміщення відсуне і результат, але матриця $(X^TX+\theta E)$ вже не буде погано обумовленою. Виявилось також, що при деякому $\theta$ ця оцінка буде мати навіть кращі властивості, ніж класична, а саме, -- меншу середньоквадратичну помилку.
\begin{theorem}
	Для гребеневої оцінки виконуються наступні властивості:
	\begin{enumerate}
		\item $\widehat{\alpha}(\theta) = (E+\theta(X^TX)^{-1})^{-1}\widehat{\alpha}$, де $\widehat{\alpha}$ -- класична оцінка, $\widehat{\alpha}=(X^TX)^{-1}X^Ty$.
		
		\item $\widehat{\alpha}(\theta) = (E-\theta(X^TX+\theta E)^{-1})^{-1}\widehat{\alpha}$.
		
		\item $M\widehat{\alpha}(\theta)=\alpha+\Delta\alpha$, де зміщення $\Delta\alpha=-\theta B\alpha$, а $B=(X^TX+\theta E)^{-1}$.
		\item $M(\theta) = M\|\widehat{\alpha}(\theta)-\alpha\|_2^2 = M \|\widehat{\alpha}(\theta)-M\widehat{\alpha}(\theta)\|_2^2 + \|\Delta\alpha\|_2^2$ (перша рівність -- просто позначення).
	\end{enumerate}
\end{theorem}
% \begin{proof} \end{proof}
% модифікована оцінка
% Застосування