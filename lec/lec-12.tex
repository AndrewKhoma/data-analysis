\subsection{Нелінійний регресійний аналіз}
Моделі $y(k)=f(x(k),\alpha)+e(k)$, $k=\overline{1,N}$, де $e(k)$ -- помилка моделі, поділяються на два підкласи:
\begin{enumerate}
	\item Внутрішньо лінійні;
	\item Внутрішньо нелінійні.
\end{enumerate}
\textit{Внутрішньо лінійні} -- це моделі які шляхом перетворень зводяться до розв’язання деякої лінійної задачі. \\
 % приклад
\textit{Внутрішньо нелінійні} -- це моделі для яких не існує шляхів зведення до лінійної моделі. \\

Оцінка $\widehat{\alpha}$ шукається як розв'язок нелінійної (бажано квадратичної) задачі \[ \widehat{\alpha}=\argmin_\alpha \Sum_k e^z(k) = \argmin_\alpha I(\alpha). \]
\section{Коваріаційний аналіз}
Треба побудувати модель залежності кількісної змінної від як від якісної так і від кількісної. Специфіка постановки задачі $\vec \xi_g \in \RR^q$ -- вектор незалежних якісних змінних, $\vec\xi\in\RR^p$ -- вектор кількісних змінних, $\eta$ -- залежна скалярна кількісна змінна. \\

На характеристику впливають як якісні так і кількісні змінні. Нехай $y(k)$ -- спостереження над $\eta$. Тоді модель класичного коваріаційного аналізу має вигляд: \[ y(k) = \Sum_{i=1}^q x_g^{(i)}(k)\alpha_g^{(i)} + \Sum_{i=1}^p x^{(i)}(k)\alpha^{(i)}+e(k)=x_g^T(k)\alpha_g + X^T(k)\alpha+e(k), \quad k = \overline{1,N}. \]
Перепишемо модель матричному вигляді: $y = X_g \alpha_g + X \alpha + e$. \\

Для знаходження оцінок параметрів моделі використовується покроковий метод найменших квадратів. \\

\textbf{Основні припущення}:
\begin{enumerate}
	\item $e \sim N(\vec0,\sigma^2E)$
	\item стовпчики матриці $X$ не залежать від умов експерименту.
	\item вважаємо, що лінійні обмеження враховані, тобто $\rang X_g = q$, $\rang X = p$.
	\item Немає обмежень на $\alpha_g$ та $\alpha$.
\end{enumerate}
Для розв’язку задачі коваріаційного аналізу, враховуючи структуру моделі використовують покроковий метод найменших квадратів:
\subsection{Двокроковий метод найменших квадратів}
\begin{enumerate}
	\item Нехай $\alpha = \theta$, тоді маємо задачу дисперсійного аналізу: знаходимо оцінку $\widehat{\alpha}_g$ методом найменших квадратів: $\widehat{\alpha}_g(\theta)=(X_g^TX_g)^{-1}X_g^Ty$ також залишкову суму квадратів $\text{CK}_e(\theta)=y^TQy$, де $Q=E-X_g(X_g^TX_g)^{-1}X_g^T$.
	\item Заміняємо в $y$ на $y-X\alpha$: $\text{CK}_e(\alpha)=(y-X\alpha)^TQ(y-X\alpha)$. Оцінка \[ \widehat{\alpha}_g = (X^TQX)^{-1}X^TQy, \quad \text{СК}_q(\widehat{\alpha})=(y-X\widehat{\alpha})^TQ(y-X\widehat{\alpha}) \]
	\item Замінюємо $y$ на $y-X\widehat{\alpha}$: \[ \widehat{\alpha}_g(\widehat{\alpha}) = (X_g^T)^{-1}X_g^T(y-X\widehat{\alpha}). \]
\end{enumerate}
\section{Аналіз часових рядів}
Нехай $\eta_t$ -- випадковий процес з дискретним часом, де $t\in I=\{1,2,3,\ldots\}$, або $=\{\ldots,-2,-1,0,1,2\ldots\}$, $y_t$ -- спостереження над цим процесом $t\in I$, тобто конкретна реалізація. \\

Така послідовність спостережень в часі називається \textit{часовим рядом} (інколи $\eta_t$ називається часовим рядом). \\

В якості математичної моделі для часового ряду будемо використовувати наступне: \[ \eta_t = f(t) + \xi_t, \quad t \in I, \] де $f(t)$ -- детермінована систематична складова часового ряду $\xi_t$ -- стохастична складова часового ряду $f(t)$ ще називається \textit{трендом}. \\

Якщо тренд можна апроксимувати на деякому класі параметричних функцій, то використовуються параметричні методи. \\

Якщо є можливість апроксимувати поліномом високого степеня у околі кожної точки, то можна використати метод ковзного середнього. 
\subsection{Часові ряди з поліномінальними трендами}
Нехай відомо, що для часового ряду $\eta_t = f(t) + \xi_t$, $t\in I$ функцію тренда можна апроксимувати поліномом високого степеня. \\

Відносно $\xi_t$ справедливо
\begin{enumerate}
	\item $\forall t:\exists M\xi_t$;
	\item $\forall t\ne s:\exists M\xi_t\xi_s$.
\end{enumerate}
Задача полягає у:
\begin{enumerate}
	\item знаходженні оцінок параметрів апроксимуючого полінома;
	\item перевірка коефіцієнтів апроксимуючого полінома на значимість;
	\item визначити порядок апроксимуючого полінома.
\end{enumerate}